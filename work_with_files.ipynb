{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном ноутбуке реализовано:\n",
    "1. проходка по директории с формированием таблицы метаданных\n",
    "2. определение из двух директорий одинаковых и различающихся файлов\n",
    "3. экспорт результатов в форматы .csv, .xlsx\n",
    "4. создано интерфейса с подключеными вышеперечисленными действиями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проходка по директории реализована с помощью хранения информации в кэше, выгрузки в реляционную базу данных SQLite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Установка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cryptohashNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading cryptohash-1.0.5.tar.gz (2.0 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: cryptohash\n",
      "  Building wheel for cryptohash (pyproject.toml): started\n",
      "  Building wheel for cryptohash (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for cryptohash: filename=cryptohash-1.0.5-py3-none-any.whl size=3404 sha256=b226a63ce5e89dea46bdd45924ec6cf5eed51cceab020704157fcc5bda208bb8\n",
      "  Stored in directory: c:\\users\\borisovams\\appdata\\local\\pip\\cache\\wheels\\20\\81\\0c\\c51997bbea0b4fc9d7364c679fc1d858f420e0e54267dc1a29\n",
      "Successfully built cryptohash\n",
      "Installing collected packages: cryptohash\n",
      "Successfully installed cryptohash-1.0.5\n"
     ]
    }
   ],
   "source": [
    "pip install cryptohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install patool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\test\\\\vscode'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(r'D:\\\\test\\\\vscode')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyInstaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyInstaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'PyInstaller' from 'c:\\\\Users\\\\BorisovaMS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\PyInstaller\\\\__init__.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyInstaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auto' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\test\\vscode\\work_with_files_000.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000084?line=0'>1</a>\u001b[0m auto\u001b[39m-\u001b[39mpy\u001b[39m-\u001b[39mto\u001b[39m-\u001b[39mexe\n",
      "\u001b[1;31mNameError\u001b[0m: name 'auto' is not defined"
     ]
    }
   ],
   "source": [
    "auto-py-to-exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BorisovaMS\\AppData\\Local\\Temp\\ipykernel_7260\\3359918734.py:10: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os.path as path\n",
    "import time\n",
    "import datetime\n",
    "import hashlib\n",
    "\n",
    "from pandas.util.testing import assert_frame_equal\n",
    "import openpyxl\n",
    "\n",
    "import timeit\n",
    "from zipfile import ZipFile\n",
    "from rarfile import RarFile\n",
    "import patoolib\n",
    "import py7zr\n",
    "\n",
    "from io import DEFAULT_BUFFER_SIZE\n",
    "\n",
    "import PySimpleGUI as sg\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\test\\\\vscode'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Получение файловой сводки по директории"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Старый вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403 ms ± 19.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df1 = create_df_with_metadata(r\"01_INCOMING_FOLDER\", 'SHA256')\n",
    "df2 = create_df_with_metadata(r\"02_DATABASE_FOLDER\", 'SHA256')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция получения метаданных файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need full file path\n",
    "def metadata_file(path_file):\n",
    "    \"\"\"\n",
    "    path_file ('str'): full path files\n",
    "    \n",
    "    This function return metadata in tuple - name file, size, path file, date and extinsion. All data is 'str'.\n",
    "    \n",
    "    \"\"\"\n",
    "    basepath, filename = os.path.split(path_file) #path file and name file with extinsion\n",
    "    name_file, extension = os.path.splitext(filename)# name and extension file\n",
    "    extension = extension[1:]\n",
    "    metadata = os.stat(os.path.abspath(path_file))\n",
    "    file_size = metadata.st_size #size file\n",
    "\n",
    "    #create date file\n",
    "    full_time = time.localtime(metadata.st_mtime) \n",
    "    data_time = datetime.datetime(full_time.tm_year, full_time.tm_mon, full_time.tm_mday, full_time.tm_hour, full_time.tm_min, full_time.tm_sec) \n",
    "    date_create_file = data_time.strftime(\"%d-%m-%Y %H:%M:%S\") # date - day-month-year hours:minutes:seconds\n",
    "    \n",
    "    new_tup = (name_file, file_size, basepath, date_create_file, extension)\n",
    "\n",
    "    return new_tup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вторая версия (ускоренная)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция обхода директории с составлением таблицы метаданных и списком длинных путей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_with_metadata(pathname):\n",
    "    \"\"\"\n",
    "    pathname ('str') - file path\n",
    "\n",
    "    Function run by path and return list with tuples metadata files and list with long path. If archive in path - function open archive. but don`t extract.\n",
    "    \"\"\"\n",
    "    data_metafiles_data = []\n",
    "    list_long_ways = []\n",
    "    for path, dirs, files in os.walk(pathname):\n",
    "        print(path)\n",
    "        if len(path) > 240: #find long way\n",
    "            list_long_ways += [path]\n",
    "        for f in files:\n",
    "            #print(f) #debug\n",
    "            file_path = path + '\\\\' + f\n",
    "            \n",
    "            if len(file_path) > 240:\n",
    "                list_long_ways += [file_path]\n",
    "                continue\n",
    "\n",
    "            if re.search('.rar', f):\n",
    "                try:\n",
    "                    with RarFile(f) as rf:\n",
    "                        for file_path in rf.namelist():\n",
    "                            if file_path[-1] == '\\\\' and file_path[-1] == '/' and len(path + file_path) > 250: #if file_path is path\n",
    "                                list_long_ways += [path + file_path]\n",
    "                                continue\n",
    "                            if file_path[-1] != '\\\\' and file_path[-1] != '/': #if file_path is file, not way\n",
    "                                data_metafiles_data += [metadata_file(os.path.abspath(os.path.join(path, file_path)))]\n",
    "                except:\n",
    "                    if file_path not in list_long_ways and len(file_path) > 240:\n",
    "                        list_long_ways += [file_path]\n",
    "\n",
    "            if re.search('.zip', f):\n",
    "                try:\n",
    "                    with RarFile(f) as rf:\n",
    "                        for file_path in rf.namelist():\n",
    "                            if file_path[-1] == '\\\\' and file_path[-1] == '/' and len(path + file_path) > 250: #if file_path is path\n",
    "                                    list_long_ways += [path + file_path]\n",
    "                            if file_path[-1] != '\\\\' and file_path[-1] != '/': #if file_path is file, not way\n",
    "                                data_metafiles_data += [metadata_file(os.path.abspath(os.path.join(path, file_path)))]\n",
    "                except:\n",
    "                    if file_path not in list_long_ways and len(file_path) > 240:\n",
    "                        list_long_ways += [file_path]\n",
    "\n",
    "            if re.search('.7z', f):\n",
    "                try:\n",
    "                    with py7zr.SevenZipFile(f, 'r') as zip7:\n",
    "                        for fname, bio in zip7.readall().items():\n",
    "                            if len(fname) > 250:\n",
    "                                list_long_ways += [fname]\n",
    "                            data_metafiles_data += [metadata_file(os.path.abspath(os.path.join(path, fname)))]\n",
    "                except:\n",
    "                    if file_path not in list_long_ways and len(file_path) > 240:\n",
    "                        list_long_ways += [file_path]\n",
    "            \n",
    "            else:\n",
    "                data_metafiles_data += [metadata_file(file_path)]\n",
    "\n",
    "    return data_metafiles_data, list_long_ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df1, df_long_paths1 = df_with_metadata(r\"01_INCOMING_FOLDER\")\n",
    "df2, df_long_paths2 = df_with_metadata(r\"02_DATABASE_FOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "df_with_metafiles1, long_paths1 = df_with_metadata(r\"\\\\10.253.30.242\\\\00_DataBank_Share\\\\01_INCOMING_FOLDER\")\n",
    "df_with_metafiles2, long_paths2 = df_with_metadata(r\"\\\\10.253.30.242\\\\00_DataBank_Share\\\\02_DATABASE_FOLDER\")\n",
    "\n",
    "df1 = pd.DataFrame([*df_with_metafiles1], columns=['filename', 'file_size', 'dirname', 'date_create_file', 'extension'])\n",
    "df2  = pd.DataFrame([*df_with_metafiles2], columns=['filename', 'file_size', 'dirname', 'date_create_file', 'extension'])\n",
    "\n",
    "df1_long_paths = pd.DataFrame([*long_paths1], columns=['dirname']).drop_duplicates().reset_index(drop=True)\n",
    "df2_long_paths  = pd.DataFrame([*long_paths2], columns=['dirname']).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариант с SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_with_metadata(pathname):\n",
    "    \"\"\"\n",
    "    pathname ('str') - file path\n",
    "    data_metafiles_data ('str') - dataframe with metadata files\n",
    "\n",
    "    Function run by path and return list with tuples metadata files and list with long path. If archive in path - function open archive. but don`t extract.\n",
    "    \"\"\"\n",
    "    data_metafiles_data = []\n",
    "    list_long_ways = []\n",
    "\n",
    "    for path, dirs, files in os.walk(pathname):\n",
    "        \n",
    "\n",
    "        print(path)\n",
    "        if len(path) > 240: #find long way\n",
    "            list_long_ways += [path]\n",
    "        for f in files:\n",
    "            #print(f) #debug\n",
    "            file_path = path + '\\\\' + f\n",
    "            \n",
    "            if len(file_path) > 240:\n",
    "                list_long_ways += [file_path]\n",
    "                continue\n",
    "\n",
    "            if re.search('.rar', f):\n",
    "                try:\n",
    "                    with RarFile(f) as rf:\n",
    "                        for file_path in rf.namelist():\n",
    "                            if file_path[-1] == '\\\\' and file_path[-1] == '/' and len(path + file_path) > 250: #if file_path is path\n",
    "                                list_long_ways += [path + file_path]\n",
    "                                continue\n",
    "                            if file_path[-1] != '\\\\' and file_path[-1] != '/': #if file_path is file, not way\n",
    "                                data_metafiles_data += [metadata_file(os.path.abspath(os.path.join(path, file_path)))]\n",
    "                except:\n",
    "                    if file_path not in list_long_ways and len(file_path) > 240:\n",
    "                        list_long_ways += [file_path]\n",
    "\n",
    "            if re.search('.zip', f):\n",
    "                try:\n",
    "                    with RarFile(f) as rf:\n",
    "                        for file_path in rf.namelist():\n",
    "                            if file_path[-1] == '\\\\' and file_path[-1] == '/' and len(path + file_path) > 250: #if file_path is path\n",
    "                                    list_long_ways += [path + file_path]\n",
    "                            if file_path[-1] != '\\\\' and file_path[-1] != '/': #if file_path is file, not way\n",
    "                                data_metafiles_data += [metadata_file(os.path.abspath(os.path.join(path, file_path)))]\n",
    "                except:\n",
    "                    if file_path not in list_long_ways and len(file_path) > 240:\n",
    "                        list_long_ways += [file_path]\n",
    "\n",
    "            if re.search('.7z', f):\n",
    "                try:\n",
    "                    with py7zr.SevenZipFile(f, 'r') as zip7:\n",
    "                        for fname, bio in zip7.readall().items():\n",
    "                            if len(fname) > 250:\n",
    "                                list_long_ways += [fname]\n",
    "                            data_metafiles_data += [metadata_file(os.path.abspath(os.path.join(path, fname)))]\n",
    "                except:\n",
    "                    if file_path not in list_long_ways and len(file_path) > 240:\n",
    "                        list_long_ways += [file_path]\n",
    "            \n",
    "            else:\n",
    "                data_metafiles_data += [metadata_file(file_path)]\n",
    "        \n",
    "        if len(data_metafiles_data) > 500000: #this quantity fits in memory\n",
    "            conn = sqlite3.connect(r\"d:\\\\test\\\\vscode\\\\aaa.db\")\n",
    "            cur = conn.cursor()\n",
    "            cur.executemany(\"INSERT INTO summary_data(file_name, file_size, dirname, date_create_file, extension) VALUES(?, ?, ?, ?, ?);\", data_metafiles_data)\n",
    "            conn.commit()\n",
    "            data_metafiles_data = []\n",
    "\n",
    "    conn = sqlite3.connect(r\"d:\\\\test\\\\vscode\\\\aaa.db\")\n",
    "    cur = conn.cursor()\n",
    "    cur.executemany(\"INSERT INTO long_paths(dirname) VALUES(?);\", [tuple([long_dir]) for long_dir in list_long_ways])\n",
    "    conn.commit()\n",
    "\n",
    "    df1 = pd.read_sql_query(\"SELECT * FROM summary_data\", conn)\n",
    "    df1_long_paths = pd.read_sql_query(\"SELECT * FROM long_paths\", conn)\n",
    "    \n",
    "    cur.execute(\"DELETE FROM summary_data;\")\n",
    "    cur.execute(\"DELETE FROM long_paths;\")\n",
    "    conn.commit()\n",
    "    return df1, df1_long_paths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(r\"d:\\\\test\\\\vscode\\\\aaa.db\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT * FROM summary_data;\")\n",
    "all_results = cur.fetchall()\n",
    "print(len(all_results))\n",
    "for row_data in all_results:\n",
    "    print(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"DELETE FROM summary_data;\")\n",
    "cur.execute(\"DELETE FROM long_paths;\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "Incorrect number of bindings supplied. The current statement uses 0, and there are 123 supplied.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32md:\\test\\vscode\\work_with_files_000.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=0'>1</a>\u001b[0m conn \u001b[39m=\u001b[39m sqlite3\u001b[39m.\u001b[39mconnect(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39md:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtest\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mvscode\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39maaa.db\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=1'>2</a>\u001b[0m cur \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=2'>3</a>\u001b[0m cur\u001b[39m.\u001b[39;49mexecute(\u001b[39m\"\"\"\u001b[39;49m\u001b[39m CREATE TABLE IF NOT EXISTS summary_data (\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=3'>4</a>\u001b[0m \u001b[39m    file_id INTEGER PRIMARY KEY AUTOINCREMENT,\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=4'>5</a>\u001b[0m \u001b[39m    file_name VARCHAR(150),\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=5'>6</a>\u001b[0m \u001b[39m    file_size VARCHAR(50),\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=6'>7</a>\u001b[0m \u001b[39m    dirname VARCHAR(270),\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=7'>8</a>\u001b[0m \u001b[39m    date_create_file VARCHAR(20),\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=8'>9</a>\u001b[0m \u001b[39m    extension VARCHAR(10)\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=9'>10</a>\u001b[0m \u001b[39m    )\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=10'>11</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=11'>12</a>\u001b[0m     \u001b[39m\"\"\" CREATE TABLE IF NOT EXISTS long_paths (\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=12'>13</a>\u001b[0m \u001b[39m    long_path_id INTEGER PRIMARY KEY AUTOINCREMENT,\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=13'>14</a>\u001b[0m \u001b[39m    dirname VARCHAR(270)\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=14'>15</a>\u001b[0m \u001b[39m    )\"\"\"\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/test/vscode/work_with_files_000.ipynb#ch0000024?line=15'>16</a>\u001b[0m conn\u001b[39m.\u001b[39mcommit()\n",
      "\u001b[1;31mProgrammingError\u001b[0m: Incorrect number of bindings supplied. The current statement uses 0, and there are 123 supplied."
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(r\"d:\\\\test\\\\vscode\\\\aaa.db\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\" CREATE TABLE IF NOT EXISTS summary_data (\n",
    "    file_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    file_name VARCHAR(150),\n",
    "    file_size VARCHAR(50),\n",
    "    dirname VARCHAR(270),\n",
    "    date_create_file VARCHAR(20),\n",
    "    extension VARCHAR(10)\n",
    "    )\"\"\",\n",
    "    \n",
    "    \"\"\" CREATE TABLE IF NOT EXISTS long_paths (\n",
    "    long_path_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    dirname VARCHAR(270)\n",
    "    )\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df1, df_long_paths1 = df_with_metadata(r\"01_INCOMING_FOLDER\")\n",
    "df2, df_long_paths2 = df_with_metadata(r\"02_DATABASE_FOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "df_with_metafiles1, long_paths1 = df_with_metadata(r\"01_INCOMING_FOLDER\")\n",
    "df_with_metafiles2, long_paths2 = df_with_metadata(r\"02_DATABASE_FOLDER\")\n",
    "\n",
    "df1 = pd.DataFrame([*df_with_metafiles1], columns=['filename', 'file_size', 'dirname', 'date_create_file', 'extension'])\n",
    "df2  = pd.DataFrame([*df_with_metafiles2], columns=['filename', 'file_size', 'dirname', 'date_create_file', 'extension'])\n",
    "\n",
    "df1_long_paths = pd.DataFrame([*long_paths1], columns=['dirname']).drop_duplicates().reset_index(drop=True)\n",
    "df2_long_paths  = pd.DataFrame([*long_paths2], columns=['dirname']).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариант с clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2, df_long_paths2 = df_with_metadata(r\"GIRS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_with_metafiles1, long_paths1 = df_with_metadata(r\"Z:\\\\Operational_Data\\\\GIRS\")\n",
    "df_with_metafiles2, long_paths2 = df_with_metadata(r\"GIRS\")\n",
    "\n",
    "#df1 = pd.DataFrame([*df_with_metafiles1], columns=['filename', 'file_size', 'dirname', 'date_create_file', 'extension'])\n",
    "df2  = pd.DataFrame([*df_with_metafiles2], columns=['filename', 'file_size', 'dirname', 'date_create_file', 'extension'])\n",
    "\n",
    "#df1_long_paths = pd.DataFrame([*long_paths1], columns=['dirname']).drop_duplicates().reset_index(drop=True)\n",
    "df2_long_paths  = pd.DataFrame([*long_paths2], columns=['dirname']).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск одинаковых и разных файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 1: name1=name2, size1=size2, date1=date2 => may be same if hash1=hash2 or different if hash1 != hash2\n",
    "#pd.set_option('max_colwidth', 25)\n",
    "#pd.set_option('display.width', 200)\n",
    "df_for_hash = df1.merge(df2, how = 'inner' ,indicator=False, on=['filename', 'file_size', 'date_create_file'])\n",
    "#df_for_hash = df_for_hash[['filename', 'file_size','date_create_file', 'dirname_x',  'extension_x','dirname_y', 'extension_y']]\n",
    "df_for_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(file):\n",
    "    \"\"\"\n",
    "    file ('str') - full path with file mane and extention\n",
    "\n",
    "    Function return hash file and full path with file mane and extention\n",
    "    \"\"\"\n",
    "    if file[4] == 'SHA256':\n",
    "        sha = hashlib.sha256()\n",
    "    if file[4] == 'SHA1':\n",
    "        sha = hashlib.sha1()\n",
    "    if file[4] == 'MD5':\n",
    "        sha = hashlib.md5()\n",
    "    \n",
    "    with open(file[0], mode='rb') as fl:\n",
    "        chunk = fl.read(DEFAULT_BUFFER_SIZE)\n",
    "        while chunk:\n",
    "            sha.update(chunk)\n",
    "            chunk = fl.read(DEFAULT_BUFFER_SIZE)\n",
    "    return sha.hexdigest(), file[0], file[1], file[2], file[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "if __name__ == '__main__':\n",
    "    x_files = [(os.path.abspath(way + '\\\\' + name + '.' + extention), size, date, type_df, type_hash) for way, name, extention, size, date, type_df, type_hash in zip(df_for_hash.dirname_x, df_for_hash.filename, df_for_hash.extension_x, df_for_hash.file_size, df_for_hash.date_create_file, 'x'*len(df_for_hash.dirname_x.values), ['SHA256']*len(df_for_hash.dirname_x.values))]\n",
    "    y_files = [(os.path.abspath(way + '\\\\' + name + '.' + extention), size, date, type_df, type_hash) for way, name, extention, size, date, type_df, type_hash in zip(df_for_hash.dirname_y, df_for_hash.filename, df_for_hash.extension_y, df_for_hash.file_size, df_for_hash.date_create_file, 'y'*len(df_for_hash.dirname_y.values), ['SHA256']*len(df_for_hash.dirname_y.values))]\n",
    "    files = [(file, size, date, type_df, type_hash) for file, size, date, type_df, type_hash in [*x_files, *y_files]]\n",
    "\n",
    "    number_of_workers = os.cpu_count()\n",
    "\n",
    "    with ThreadPool(number_of_workers) as pool:\n",
    "        files_hash = pool.map(get_hash, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение по hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_x = []\n",
    "list_y = []\n",
    "for hash, way, size, date, type_df in files_hash:\n",
    "    file_path, full_filename = os.path.split(way)\n",
    "    filename, filextention = os.path.splitext(full_filename)\n",
    "    extension = filextention[1:]\n",
    "    row = (filename, size, date, file_path, extension, hash)\n",
    "\n",
    "    if type_df == 'x':\n",
    "        list_x += [row]\n",
    "\n",
    "    if type_df == 'y':\n",
    "        list_y += [row]\n",
    "df_x = pd.DataFrame([*list_x], columns=['filename', 'file_size', 'date_create_file', 'dirname', 'extension', 'hash_file'])\n",
    "df_y = pd.DataFrame([*list_y], columns=['filename', 'file_size', 'date_create_file', 'dirname',  'extension', 'hash_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 1: name1=name2, size1=size2, date1=date2 => may be same if hash1=hash2 or different if hash1 != hash2\n",
    "df_same = df_x.merge(df_y, how = 'inner' , indicator=False, on=['filename', 'file_size','date_create_file','hash_file'])\n",
    "#df_first_case_same.drop_duplicates(subset=['filename', 'file_size','date_create_file','hash_file']).reset_index(drop=True)\n",
    "pd.set_option('max_colwidth', 25)\n",
    "pd.set_option('display.width', 200)\n",
    "df_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_str_same = pd.DataFrame([((filename+str(file_size)+date_create_file+dirname_x+extension_x),(filename+str(file_size)+date_create_file+dirname_y+extension_y)) \n",
    "        for filename, file_size, date_create_file,dirname_x, extension_x, dirname_y, extension_y \n",
    "        in zip(df_same.filename, df_same.file_size, df_same.date_create_file, df_same.dirname_x, df_same.extension_x, df_same.dirname_y, df_same.extension_y)], columns=['x', 'y'])\n",
    "df_str_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_str_df1 = pd.DataFrame([(filename+str(file_size)+date_create_file+os.path.abspath(dirname)+extension) for filename, file_size, date_create_file, dirname, extension \n",
    "        in zip(df1.filename, df1.file_size, df1.date_create_file, df1.dirname, df1.extension)], columns=['df1'])\n",
    "df_str_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_str_df2 = pd.DataFrame([(filename+str(file_size)+date_create_file+os.path.abspath(dirname)+extension) for filename, file_size, date_create_file, dirname, extension \n",
    "        in zip(df2.filename, df2.file_size, df2.date_create_file, df2.dirname, df2.extension)], columns=['df2'])\n",
    "df_str_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff1 = df1[~df_str_df1.df1.isin(df_str_same.x)].reset_index(drop=True)\n",
    "df_diff1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff2 = df2[~df_str_df2.df2.isin(df_str_same.y)].reset_index(drop=True)\n",
    "df_diff2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выгрузка результатов в документы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\test\\\\vscode'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(r'd:\\\\test\\\\vscode')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('./GIRS_INCOMING.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('./GIRS_DATABASE.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "df_same.to_csv('./same_files.csv', encoding='cp1251', sep=';')\n",
    "df_diff.to_csv('./different_files.csv', encoding='cp1251', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to xlsx\n",
    "with pd.ExcelWriter(\"./summary_files.xlsx\") as writer:\n",
    "    df_same.to_excel(writer, sheet_name='same_files')\n",
    "    df_diff.to_excel(writer, sheet_name='different_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подключение графического интерфейса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.theme_previewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LONG WAYS DATAFRAME AND METADATA BY PATHS\n",
    "def metadata_file(path_file):\n",
    "    \"\"\"\n",
    "    path_file ('str'): full path files\n",
    "    \n",
    "    This function return metadata in tuple - name file, size, path file, date and extinsion. All data is 'str'.\n",
    "    \n",
    "    \"\"\"\n",
    "    basepath, filename = os.path.split(path_file)\n",
    "    name_file, extension = os.path.splitext(filename)# name and extension file\n",
    "    extension = extension[1:]\n",
    "    metadata = os.stat(os.path.abspath(path_file))\n",
    "    file_size = metadata.st_size #size file\n",
    "\n",
    "    #create date file\n",
    "    full_time = time.localtime(metadata.st_mtime) \n",
    "    data_time = datetime.datetime(full_time.tm_year, full_time.tm_mon, full_time.tm_mday, full_time.tm_hour, full_time.tm_min, full_time.tm_sec) \n",
    "    date_create_file = data_time.strftime(\"%d-%m-%Y %H:%M:%S\") # date - day-month-year hours:minutes:seconds\n",
    "    \n",
    "    new_tup = (name_file, file_size, basepath, date_create_file, extension)\n",
    "\n",
    "    return new_tup\n",
    "\n",
    "def df_with_metadata(pathname):\n",
    "    data_metafiles_data = []\n",
    "    list_long_ways = []\n",
    "    \"\"\"\n",
    "    pathname ('str') - file path\n",
    "\n",
    "    Function run by path and return list with tuples metadata files and list with long path. If archive in path - function open archive. but don`t extract.\n",
    "    \"\"\"\n",
    "    for path, dirs, files in os.walk(pathname):\n",
    "        print(path)\n",
    "        if len(path) > 240: #find long way\n",
    "            list_long_ways += [path]\n",
    "        for f in files:\n",
    "            #print(f) #debug\n",
    "            file_path = path + '\\\\' + f\n",
    "            \n",
    "            if len(file_path) > 240:\n",
    "                list_long_ways += [file_path]\n",
    "                continue\n",
    "\n",
    "            if re.search('.rar', f):\n",
    "                try:\n",
    "                    with RarFile(f) as rf:\n",
    "                        for file_path in rf.namelist():\n",
    "                            if file_path[-1] == '\\\\' and file_path[-1] == '/' and len(path + file_path) > 250: #if file_path is path\n",
    "                                list_long_ways += [path + file_path]\n",
    "                                continue\n",
    "                            if file_path[-1] != '\\\\' and file_path[-1] != '/': #if file_path is file, not way\n",
    "                                data_metafiles_data += [metadata_file(os.path.abspath(os.path.join(path, file_path)))]\n",
    "                except:\n",
    "                    if file_path not in list_long_ways and len(file_path) > 240:\n",
    "                        list_long_ways += [file_path]\n",
    "\n",
    "            if re.search('.zip', f):\n",
    "                try:\n",
    "                    with RarFile(f) as rf:\n",
    "                        for file_path in rf.namelist():\n",
    "                            if file_path[-1] == '\\\\' and file_path[-1] == '/' and len(path + file_path) > 250: #if file_path is path\n",
    "                                    list_long_ways += [path + file_path]\n",
    "                            if file_path[-1] != '\\\\' and file_path[-1] != '/': #if file_path is file, not way\n",
    "                                data_metafiles_data += [metadata_file(os.path.abspath(os.path.join(path, file_path)))]\n",
    "                except:\n",
    "                    if file_path not in list_long_ways and len(file_path) > 240:\n",
    "                        list_long_ways += [file_path]\n",
    "\n",
    "            if re.search('.7z', f):\n",
    "                try:\n",
    "                    with py7zr.SevenZipFile(f, 'r') as zip7:\n",
    "                        for fname, bio in zip7.readall().items():\n",
    "                            if len(fname) > 250:\n",
    "                                list_long_ways += [fname]\n",
    "                            data_metafiles_data += [metadata_file(os.path.abspath(os.path.join(path, fname)))]\n",
    "                except:\n",
    "                    if file_path not in list_long_ways and len(file_path) > 240:\n",
    "                        list_long_ways += [file_path]\n",
    "            \n",
    "            else:\n",
    "                data_metafiles_data += [metadata_file(file_path)]\n",
    "\n",
    "    return data_metafiles_data, list_long_ways\n",
    "\n",
    "\n",
    "#HASH FILE\n",
    "def get_hash(file):\n",
    "    \"\"\"\n",
    "    file ('str') - full path with file mane and extention\n",
    "\n",
    "    Function return hash file and full path with file mane and extention\n",
    "    \"\"\"\n",
    "    if file[4] == 'SHA256':\n",
    "        sha = hashlib.sha256()\n",
    "    if file[4] == 'SHA1':\n",
    "        sha = hashlib.sha1()\n",
    "    if file[4] == 'MD5':\n",
    "        sha = hashlib.md5()\n",
    "    \n",
    "    with open(file[0], mode='rb') as fl:\n",
    "        chunk = fl.read(DEFAULT_BUFFER_SIZE)\n",
    "        while chunk:\n",
    "            sha.update(chunk)\n",
    "            chunk = fl.read(DEFAULT_BUFFER_SIZE)\n",
    "    return sha.hexdigest(), file[0], file[1], file[2], file[3]\n",
    "\n",
    "\n",
    "def get_same_and_diff_files(df1, df2, df_for_hash, type_hash):\n",
    "    if __name__ == '__main__':\n",
    "        x_files = [(os.path.abspath(way + '\\\\' + name + '.' + extention), size, date, type_df, type_hash) for way, name, extention, size, date, type_df, type_hash in zip(df_for_hash.dirname_x, df_for_hash.filename, df_for_hash.extension_x, df_for_hash.file_size, df_for_hash.date_create_file, 'x'*len(df_for_hash.dirname_x.values), [type_hash]*len(df_for_hash.dirname_x.values))]\n",
    "        y_files = [(os.path.abspath(way + '\\\\' + name + '.' + extention), size, date, type_df, type_hash) for way, name, extention, size, date, type_df, type_hash in zip(df_for_hash.dirname_y, df_for_hash.filename, df_for_hash.extension_y, df_for_hash.file_size, df_for_hash.date_create_file, 'y'*len(df_for_hash.dirname_y.values), [type_hash]*len(df_for_hash.dirname_y.values))]\n",
    "        files = [(file, size, date, type_df, type_hash) for file, size, date, type_df, type_hash in [*x_files, *y_files]]\n",
    "\n",
    "        number_of_workers = os.cpu_count()\n",
    "\n",
    "        print('Programm count hash files')\n",
    "        with ThreadPool(number_of_workers) as pool:\n",
    "            files_hash = pool.map(get_hash, files)\n",
    "\n",
    "        #Search same and difference files with hash\n",
    "        print('Search same and difference files, part 2 of 2')\n",
    "        print('Please wait')\n",
    "        list_x = []\n",
    "        list_y = []\n",
    "        for hash, way, size, date, type_df in files_hash:\n",
    "            file_path, full_filename = os.path.split(way)\n",
    "            filename, filextention = os.path.splitext(full_filename)\n",
    "            extension = filextention[1:]\n",
    "            row = (filename, size, date, file_path, extension, hash)\n",
    "\n",
    "            if type_df == 'x':\n",
    "                list_x += [row]\n",
    "\n",
    "            if type_df == 'y':\n",
    "                list_y += [row]\n",
    "        df_x = pd.DataFrame([*list_x], columns=['filename', 'file_size', 'date_create_file', 'dirname', 'extension', 'hash_file'])\n",
    "        df_y = pd.DataFrame([*list_y], columns=['filename', 'file_size', 'date_create_file', 'dirname',  'extension', 'hash_file'])\n",
    "\n",
    "        df_same = df_x.merge(df_y, how = 'inner' , indicator=False, on=['filename', 'file_size','date_create_file','hash_file'])\n",
    "\n",
    "        #different files. Check rows tith same name file, size, date, dirname and extension. Create srt mask for df1 and df2\n",
    "        df_str_same = pd.DataFrame([((filename+str(file_size)+date_create_file+dirname_x+extension_x),(filename+str(file_size)+date_create_file+dirname_y+extension_y)) \n",
    "        for filename, file_size, date_create_file,dirname_x, extension_x, dirname_y, extension_y \n",
    "        in zip(df_same.filename, df_same.file_size, df_same.date_create_file, df_same.dirname_x, df_same.extension_x, df_same.dirname_y, df_same.extension_y)], columns=['x', 'y'])\n",
    "        \n",
    "        df_str_df1 = pd.DataFrame([(filename+str(file_size)+date_create_file+os.path.abspath(dirname)+extension) for filename, file_size, date_create_file, dirname, extension \n",
    "                in zip(df1.filename, df1.file_size, df1.date_create_file, df1.dirname, df1.extension)], columns=['df1'])\n",
    "        df_str_df2 = pd.DataFrame([(filename+str(file_size)+date_create_file+os.path.abspath(dirname)+extension) for filename, file_size, date_create_file, dirname, extension \n",
    "                in zip(df2.filename, df2.file_size, df2.date_create_file, df2.dirname, df2.extension)], columns=['df2'])\n",
    "        \n",
    "        df_diff1 = df1[~df_str_df1.df1.isin(df_str_same.x)].reset_index(drop=True)\n",
    "        df_diff2 = df2[~df_str_df2.df2.isin(df_str_same.y)].reset_index(drop=True)\n",
    "\n",
    "        return df_same, df_diff1, df_diff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.theme('LightBrown13')\n",
    "w, h = sg.Window.get_screen_size()\n",
    "\n",
    "#TAB 'Сomputing'\n",
    "tab0_layout = [[sg.Text('Hash method:'), sg.Radio('MD5','sha'), sg.Radio('SHA1','sha'), sg.Radio('SHA256','sha',default=True),\n",
    "                    sg.Text('Note:', font = (\"Arial\", 10, 'bold')), sg.Text('choose one (!) hash method, default SHA256.')], \n",
    "               [sg.Text('Extension of the results files:'), sg.Checkbox('csv', default=True), sg.Checkbox('xlsx'), \n",
    "                    sg.Text('    Note:', font = (\"Arial\", 10, 'bold')), sg.Text('choose one or all extentions. Default csv, you cannot cancel this extension.')],\n",
    "\n",
    "    [sg.Text('The initial path to save the results:'), sg.InputText(size = (65, 5), enable_events=True), sg.FolderBrowse()],\n",
    "    [sg.Text('Path 1:'), sg.InputText(key='foldername1', size = (65, 5), enable_events=True), sg.FolderBrowse(),\n",
    "         sg.Text('Path 2:'), sg.InputText(key='foldername2', size = (65, 5), enable_events=True), sg.FolderBrowse()],\n",
    "    [sg.Text('OR  (you can choose summary files)')],\n",
    "    [sg.Text('File 1:  '), sg.InputText(size = (65, 5), enable_events=True), sg.FileBrowse(), sg.Text('File 2: '), sg.InputText(size = (65, 5), enable_events=True), sg.FileBrowse()],\n",
    "    \n",
    "    [sg.Frame('Output', layout = [[sg.Output(key='-output-', size=(w, h - (h - h//35))) ]]) ],\n",
    "    [sg.Frame('Progress', layout = [[sg.ProgressBar(7, orientation='h', size=(30, 10), key='progressbar')]]) ],\n",
    "    [sg.Submit('Start'),sg.Cancel()]]\n",
    "\n",
    "#TAB 'Long paths'\n",
    "headings_ways = ['directory name']\n",
    "ways_cols_width = [w//20]\n",
    "tab1_layout = [[sg.Text('This tab contain long paths by two path. Programm don`t analyse files with long path.')],\n",
    "                [sg.Text('Path 1:'), sg.Text(key='foldername1_long_paths'), sg.Text('Path 2:'), sg.Text(key='foldername2_long_paths')],\n",
    "                [sg.Table(values=[], headings=headings_ways, col_widths=ways_cols_width, auto_size_columns=False, enable_events=True, num_rows= w//38,\n",
    "                    display_row_numbers=True, justification='right', key='table_long_way1', vertical_scroll_only=False),\n",
    "                sg.Table(values=[], headings=headings_ways, col_widths=ways_cols_width, auto_size_columns=False, enable_events=True, num_rows= w//38,\n",
    "                    display_row_numbers=True, justification='right', key='table_long_way2', vertical_scroll_only=False)]\n",
    "]\n",
    "\n",
    "#TAB 'Summary files'\n",
    "headings_data = ['filename', 'file size', 'directory name', 'date create file', 'extension']\n",
    "\n",
    "tab2_layout = [[sg.Text('This tab contain summary by two path with files.')],\n",
    "                [sg.Text('Path 1:'),  sg.Text(key='foldername1_summary'), sg.Text('Path 2:'), sg.Text(key='foldername2_summary')],\n",
    "                [sg.Table(values=[], headings=headings_data, auto_size_columns=True, num_rows= w//38,\n",
    "                    display_row_numbers=True, justification='right', key='table_data_df1', vertical_scroll_only=False),\n",
    "                sg.Table(values=[], headings=headings_data, auto_size_columns=True, num_rows= w//38,\n",
    "                    display_row_numbers=True, justification='right', key='table_data_df2', vertical_scroll_only=False)]\n",
    "]\n",
    "\n",
    "#TAB 'Same files'\n",
    "headings_same_data = ['filename', 'file size', 'date create file', 'directory name path1', 'extention path1', 'hash file', 'directory name path2', 'extention path2']\n",
    "\n",
    "tab3_layout = [[sg.Text('Same files from two paths.')],\n",
    "                [sg.Text('    Note:', font = (\"Arial\", 10, 'bold')), sg.Text('if you want to select several lines manually, then hold down the button \"Ctrl\" and click on the line with the left mouse button.')],\n",
    "                [sg.Table(values=[], headings=headings_same_data, auto_size_columns=True, num_rows= w//48,\n",
    "                    display_row_numbers=True, justification='right', key='table_data_same', vertical_scroll_only=False)], \n",
    "                [sg.Text(key='-same_table_comm-')],\n",
    "                [sg.Text('Delete in:'), sg.Radio('Path 1','delete', default=True, key='-delete_by_path1-'), sg.Radio('Path 2','delete', key='-delete_by_path2-')],\n",
    "                #[sg.Button('Select all for delete'), sg.Button('Deselect for delete')],\n",
    "                [sg.Submit('DELETE')]\n",
    "]\n",
    "\n",
    "#TAB 'Difference files'\n",
    "headings_diff_data = ['filename', 'file size', 'directory name', 'date create file', 'extension']\n",
    "\n",
    "tab4_layout = [[sg.Text('Different files from two paths.')],\n",
    "                [sg.Text('Path for copy:'), sg.InputText(size = (65, 5), enable_events=True), sg.FolderBrowse()],\n",
    "                [sg.Text('    Note:', font = (\"Arial\", 10, 'bold')), sg.Text('if you want to select several lines manually, then hold down the button \"Ctrl\" and click on the line with the left mouse button.')],\n",
    "                [sg.Table(values=[], headings=headings_diff_data, auto_size_columns=True, num_rows= w//48,\n",
    "                    display_row_numbers=True, justification='right', key='table_data_diff1', vertical_scroll_only=False),\n",
    "                sg.Table(values=[], headings=headings_diff_data, auto_size_columns=True, num_rows= w//48,\n",
    "                    display_row_numbers=True, justification='right', key='table_data_diff2', vertical_scroll_only=False)],\n",
    "                [sg.Text(key='-diff1_table_comm-'), sg.Text(key='-diff2_table_comm-')],\n",
    "                [sg.Text('Copy from:'), sg.Radio('Path 1','copy', default=True, key='-copy_from_path1-'), sg.Radio('Path 2','copy', key='-copy_from_path2-')],\n",
    "                #[sg.Button('Select all for copy'), sg.Button('Deselect for copy')],\n",
    "                [sg.Submit('COPY'), sg.Text('Please choose path for copy.', font = (\"Arial\", 10, 'bold'), key='-diff_tables_comm-')]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "layout = [[[sg.Text('Please close all other programs on your computer. This will help to calculate everything quickly.', font = (\"Arial\", 12, 'bold'))],\n",
    "    \n",
    "    [sg.Text('1. Choose path for save compare results. The entered directories (path1 and path2) will be saved in this directory and they can be opened next time.'),\n",
    "     ],\n",
    "     [sg.Text('2. To compare files, select directories. The files of the first directory are checked (they are new), the files of the second are considered a database (they are already in the system).\\n You can select summary files if they already exist. Like with directories file 1 - incoming, file 2 - database.'),\n",
    "     ],\n",
    "     [sg.Text('Note:', font = (\"Arial\", 10, 'bold')),sg.Text('You must have access to all the selected directories (for delete and copy files).'),\n",
    "     ],\n",
    "     [sg.Text('Attention:', font = (\"Arial\", 10, 'bold')),sg.Text('If you selected a file AND directories, then the program will use the file.'),\n",
    "     ],\n",
    "     [sg.Text('3. Click \"Start\".'),\n",
    "     ],\n",
    "           sg.TabGroup([[sg.Tab('Сomputing', tab0_layout),\n",
    "                         sg.Tab('Long paths', tab1_layout, visible = False, key='long_path'),\n",
    "                         sg.Tab('Summary files', tab2_layout, visible = False, key='summary'),\n",
    "                         sg.Tab('Same files', tab3_layout, visible = False, key='same_f'),\n",
    "                         sg.Tab('Difference files', tab4_layout, visible = False, key='diff_f')]], key = '-TabGroup-', size=(w, h - (h//6)))]]\n",
    "\n",
    "window = sg.Window('Compare files by two path', layout, resizable=True, size=(1250, 750)).Finalize()\n",
    "window.Maximize()\n",
    "\n",
    "progress_bar = window['progressbar']\n",
    "\n",
    "while True:\n",
    "    event, values = window.read(timeout=10)\n",
    "    #print(event, values)\n",
    "\n",
    "    #values -  0: MD5, 1: SHA1, 2: SHA256, 3: csv, 4: xlsx, 5 :home_folder, 'foldername1': Path1, 'foldername2': Path2, 6: file1, 7: file2\n",
    "    if event in (None, 'Exit', 'Cancel'):\n",
    "        break\n",
    "    if values[5] != '' and event != 'Start':\n",
    "        home_foldername = values[5] + '/compare_files'\n",
    "        if os.path.isdir(home_foldername) != False: #if programm opened before\n",
    "            with open(home_foldername + '/cash_paths.txt', 'r') as f:\n",
    "                list_paths = f.read().split('\\n')\n",
    "                foldername1 = values['foldername1'] = list_paths[0]\n",
    "                foldername2 = values['foldername2'] = list_paths[1]\n",
    "\n",
    "                window['foldername1'].update(foldername1)\n",
    "                window['foldername2'].update(foldername2)\n",
    "        try:\n",
    "\n",
    "            if foldername1 != values['foldername1']:\n",
    "                foldername1 = values['foldername1']\n",
    "                window['foldername1'].update(foldername1)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "\n",
    "            if foldername2 != values['foldername2']:\n",
    "                foldername2 = values['foldername2']\n",
    "                window['foldername2'].update(foldername2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if values[5] == '' and event == 'Start':\n",
    "        print('Please choose the initial path to save the results.')\n",
    "    elif event == 'Start':\n",
    "        if os.path.isdir(home_foldername) == False: #if programm opened first time\n",
    "            os.mkdir(home_foldername)\n",
    "            foldername1 = values['foldername1']\n",
    "            foldername2 = values['foldername2']\n",
    "        \n",
    "        with open(home_foldername + '\\\\cash_paths.txt', 'w') as f: #save paths in file for next work\n",
    "           f.writelines(\"%s\\n\" % line for line in [foldername1, foldername2])\n",
    "\n",
    "        if foldername1 == '' and values[6] == '':\n",
    "            print('Please coose Path1 or File 1')\n",
    "\n",
    "        if foldername2 == '' and values[7] == '':\n",
    "            print('Please choose Path2 or File 2')\n",
    "\n",
    "\n",
    "        if ((foldername1 != '' or values[6] != '') and (foldername2 != '' or values[7] != '')) and (values[0] is True or values[1] is True or values[2] is True):\n",
    "            #CHECHING LONG PATHS AND GET SUMMARY PATHS\n",
    "            print('Run by:')\n",
    "            progress_bar.UpdateBar(0)\n",
    "            #for path 1\n",
    "            if values[6] == '':\n",
    "                list_df1, list_df_long_paths1 = df_with_metadata(foldername1)\n",
    "                df1  = pd.DataFrame([*list_df1], columns=['filename', 'file_size', 'dirname', 'date_create_file', 'extension'])\n",
    "                df1.to_csv(home_foldername+'\\\\all_files_path1.csv', sep=';')\n",
    "\n",
    "                df_long_paths1  = pd.DataFrame([*list_df_long_paths1], columns=['dirname']).drop_duplicates().reset_index(drop=True)\n",
    "                if len(df_long_paths1) > 0:\n",
    "                    sg.PopupOK(' The program (a) finds long way(s) in Path 1. You can see results in tab \"Long paths\".')\n",
    "                    window['table_long_way1'].update(values = [list(f) for f in df_long_paths1.values])\n",
    "                    df_long_paths1.to_csv(home_foldername+'\\\\long_ways_path1.csv', sep=';')\n",
    "            else:\n",
    "                df1 = pd.read_csv(values[6],sep=\";\")\n",
    "                df_long_paths1 = []\n",
    "\n",
    "            progress_bar.UpdateBar(1)\n",
    "            #for path 2\n",
    "            if values[7] == '':\n",
    "                list_df2, list_df_long_paths2 = df_with_metadata(foldername2)\n",
    "                df2  = pd.DataFrame([*list_df2], columns=['filename', 'file_size', 'dirname', 'date_create_file', 'extension'])\n",
    "                df2.to_csv(home_foldername+'\\\\all_files_path2.csv', sep=';')            \n",
    "                \n",
    "                df_long_paths2  = pd.DataFrame([*list_df_long_paths2], columns=['dirname']).drop_duplicates().reset_index(drop=True)\n",
    "                if len(df_long_paths2) > 0:\n",
    "                    sg.PopupOK(' The program (a) finds long way(s) in Path 2. You can see results in tab \"Long paths\".')\n",
    "                    window['table_long_way2'].update(values = [list(f) for f in df_long_paths2.values])\n",
    "                    df_long_paths2.to_csv(home_foldername +'\\\\long_ways_path2.csv', sep=';')\n",
    "            else:\n",
    "                df2 = pd.read_csv(values[7],sep=\";\")\n",
    "                df_long_paths2 = []\n",
    "\n",
    "            #activate or nor tab long path\n",
    "            if len(df_long_paths1) > 1 or len(df_long_paths2) > 1: #visible tab or not\n",
    "                window['foldername1_long_paths'].update(foldername1)\n",
    "                window['foldername2_long_paths'].update(foldername2)\n",
    "                window['long_path'].update(visible = True)\n",
    "            else:\n",
    "                window['long_path'].update(visible = False)\n",
    "\n",
    "            progress_bar.UpdateBar(2)\n",
    "            #activate tab summary\n",
    "            window['foldername1_summary'].update(foldername1)\n",
    "            window['foldername2_summary'].update(foldername2)\n",
    "            window['table_data_df1'].update(values = [list(f) for f in df1.values])\n",
    "            window['table_data_df2'].update(values = [list(f) for f in df2.values])\n",
    "            window['summary'].update(visible = True)\n",
    "             \n",
    "            \n",
    "            #SAME AND DIFFERENCE\n",
    "            progress_bar.UpdateBar(3)\n",
    "            #Search same and difference files for count hash\n",
    "            #case 1: name1=name2, size1=size2, date1=date2 => may be same if hash1=hash2 or different if hash1 != hash2\n",
    "\n",
    "            print('Search same and difference files, part 1 of 2')          \n",
    "            df_first_case = df1.merge(df2, how = 'inner' ,indicator=False, on=['filename', 'file_size', 'date_create_file'])\n",
    "            progress_bar.UpdateBar(4)\n",
    "\n",
    "            #set hash type\n",
    "            if values[2] == True:\n",
    "                type_hash = 'SHA256'\n",
    "            elif values[0] == True:\n",
    "                type_hash = 'MD5'\n",
    "            elif values[1] == True:\n",
    "                type_hash = 'SHA1'\n",
    "            print('...')\n",
    "\n",
    "\n",
    "            progress_bar.UpdateBar(5)\n",
    "            df_first_case_same, df_diff1, df_diff2 = get_same_and_diff_files(df1, df2, df_first_case, type_hash)\n",
    "            df_first_case_same.to_csv(home_foldername+'\\\\same_files_by_two_paths.csv', encoding='cp1251', sep=';')\n",
    "            df_diff1.to_csv(home_foldername+'\\\\different_files_path_1.csv', encoding='cp1251', sep=';')\n",
    "            df_diff2.to_csv(home_foldername+'\\\\different_files_path_2.csv', encoding='cp1251', sep=';') \n",
    "\n",
    "            progress_bar.UpdateBar(6)\n",
    "            if values[4] == True: #resaults to xlsx\n",
    "                try:\n",
    "                    df_first_case_same.to_excel(\"./same_files.xlsx\")\n",
    "                    with pd.ExcelWriter(\"./long_paths_files.xlsx\") as writer:\n",
    "                        df_long_paths1.to_excel(writer, sheet_name='long_paths_1')\n",
    "                        df_long_paths2.to_excel(writer, sheet_name='long_paths_2')\n",
    "                    with pd.ExcelWriter(\"./different_files.xlsx\") as writer:\n",
    "                        df_diff1.to_excel(writer, sheet_name='different_files_1')\n",
    "                        df_diff2.to_excel(writer, sheet_name='different_files_2')\n",
    "                    with pd.ExcelWriter(\"./summary_files.xlsx\") as writer:\n",
    "                        df1.to_excel(writer, sheet_name='files_paths_1')\n",
    "                        df2.to_excel(writer, sheet_name='files_paths_2')\n",
    "                except:\n",
    "                    print('Unfortunately, the program cannot upload the results to excel.')\n",
    "\n",
    "            progress_bar.UpdateBar(7)\n",
    "            window['table_data_same'].update(values = [list(f) for f in df_first_case_same.values])\n",
    "            window['same_f'].update(visible = True)\n",
    "            window['table_data_diff1'].update(values = [list(f) for f in df_diff1.values])\n",
    "            window['table_data_diff2'].update(values = [list(f) for f in df_diff2.values]) \n",
    "            window['diff_f'].update(visible = True)\n",
    "\n",
    "            print('See the results on other tabs.')\n",
    "    \n",
    "    \"\"\"\n",
    "    #buttons select all and deselect for delete\n",
    "    if event == 'Select all for delete':\n",
    "        window['-same_table_comm-'].update('select')\n",
    "    if event == 'Deselect for delete':\n",
    "        window['-same_table_comm-'].update('noselect')\n",
    "    \n",
    "    #buttons select all and deselect for copy\n",
    "    if event == 'Select all for copy':\n",
    "        window['-diff1_table_comm-'].update('select')\n",
    "        if values['-copy_from_path1-'] == True: #copy from path 1\n",
    "            #нужны значения строк по директории 1 \n",
    "            print('copy 1')\n",
    "\n",
    "        if values['-copy_from_path2-'] == True: # or copy from path 2\n",
    "            #нужны значения строк по директории 2\n",
    "            print('copy 2')\n",
    "         \n",
    "    if event == 'Deselect for copy':\n",
    "        window['-diff2_table_comm-'].update('noselect')\n",
    "        if values['-copy_from_path1-'] == True: #copy from path 1\n",
    "            #нужны значения строк по директории 1\n",
    "            print('uncopy 1')\n",
    "\n",
    "        if values['-copy_from_path2-'] == True: # or copy from path 2\n",
    "            #нужны значения строк по директории 2\n",
    "            print('uncopy 2')\n",
    "    \"\"\"\n",
    "\n",
    "    #delete files\n",
    "    if values['table_data_same'] != '' and event == 'DELETE': #delete files in tab same files\n",
    "        #window['-same_table_comm-'].update(values['table_data_same'])\n",
    "        if values['-delete_by_path1-'] == True: #delete in path 1\n",
    "            data_select_for_delete1 = [(df_first_case_same.dirname_x[row]+'\\\\'+df_first_case_same.filename[row]+'.'+df_first_case_same.extension_x[row]) for row in values['table_data_same']]\n",
    "            window['-same_table_comm-'].update(data_select_for_delete1)\n",
    "\n",
    "        if values['-delete_by_path2-'] == True: # or delete in path 2\n",
    "            data_select_for_delete2 = [(df_first_case_same.dirname_y[row]+'\\\\'+df_first_case_same.filename[row]+'.'+df_first_case_same.extension_y[row]) for row in values['table_data_same']]\n",
    "            window['-same_table_comm-'].update(data_select_for_delete2)\n",
    "\n",
    "    #copy files\n",
    "    path_for_copy = values['Browse4']\n",
    "    if values['Browse4'] != \"\":\n",
    "        window['-diff_tables_comm-'].update('')\n",
    "        if values['table_data_diff1'] != '' and event == 'COPY' and values['-copy_from_path1-'] == True:\n",
    "            data_select_for_copy1 = [(df_diff1.dirname[row]+'\\\\'+df_diff1.filename[row]+'.'+df_diff1.extension[row]) for row in values['table_data_diff1']]\n",
    "            window['-diff1_table_comm-'].update(data_select_for_copy1)\n",
    "            window['-diff2_table_comm-'].update('')\n",
    "            \"\"\"\n",
    "            for sor in data_select_for_copy1:\n",
    "                path_split = (sor).split('\\\\') #split path by folders and file\n",
    "                clearn_path_split = []\n",
    "                for part in path_split:\n",
    "                    if part != '':\n",
    "                        clearn_path_split += [part]\n",
    "                clearn_path_split = clearn_path_split[1:]#choose without first folder/disk name\n",
    "\n",
    "                path_for_copy = os.path.normpath(values['Browse4'])\n",
    "                for part in clearn_path_split:\n",
    "                    try:\n",
    "                        os.mkdir(path_for_copy+'\\\\\\\\'+part)\n",
    "                    except:\n",
    "                        if len(os.path.splitext(part)) == 2:\n",
    "                            shutil.copy(part, path_for_copy, follow_symlinks=True)\n",
    "                    path_for_copy = path_for_copy+'\\\\\\\\'+part\n",
    "\n",
    "            #window['-diff1_table_comm-'].update(values['table_data_diff1'])\n",
    "            \"\"\"\n",
    "\n",
    "        if values['table_data_diff2'] != '' and event == 'COPY' and values['-copy_from_path2-'] == True:\n",
    "            data_select_for_copy2 = [(df_diff2.dirname[row]+'\\\\'+df_diff2.filename[row]+'.'+df_diff2.extension[row]) for row in values['table_data_diff2']]\n",
    "            window['-diff2_table_comm-'].update(data_select_for_copy2)\n",
    "            window['-diff1_table_comm-'].update('')\n",
    "\n",
    "            #window['-diff2_table_comm-'].update(values['table_data_diff2'])\n",
    "\n",
    "            \n",
    "window.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью данной функции можно подбирать цвета для интерфейса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import colorchooser\n",
    "\n",
    "def pick_color():\n",
    "    color = colorchooser.askcolor(title =\"Choose color\")\n",
    "    color_me.config(bg=color[1])\n",
    "    color_me.config(text=color)\n",
    "   \n",
    " \n",
    "ws = Tk()\n",
    "ws.title('PythonGuides')\n",
    "ws.geometry('400x300')\n",
    "\n",
    "color_me = Label(\n",
    "    ws,\n",
    "    text='(217, 217, 217) #d9d9d9',\n",
    "    font = ('Times', 20),\n",
    "    relief = SOLID,\n",
    "    padx=20, \n",
    "    pady=20\n",
    ")\n",
    "color_me.pack(expand=True)\n",
    "button = Button(\n",
    "    ws, \n",
    "    text = \"Choose Color\",\n",
    "    command = pick_color,\n",
    "    padx=10,\n",
    "    pady=10,\n",
    "    font=('Times', 18),\n",
    "    bg='#4a7a8c'\n",
    "    )\n",
    "button.pack()\n",
    "\n",
    "ws.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f0731e525e7a86c173622e4bdb83527d6a26c73a7ad89a64c8412436e44842a1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
